\chapter*{Zusammenfassung}
\addcontentsline{toc}{chapter}{Zusammenfassung}

In den letzten 15 Jahren wurden verschiedene Aspekte von Metadatenqualität untersucht. Wissenschaftler haben dabei etablierte Metriken auf verschiedenen Metadatenkollektionen angewendet. Gemeinsam ist diesen Forschungsprojekten, das die Tools, die zur Messung nötig sind, häufig nicht darauf ausgelegt sind wiederverwendet  zu werden. Die vorliegende Arbeit, die sich hauptsächlich mit einer spezielle Metadatenkollektion - Europeana - beschäftigt, untersucht praktische Aspekte der Bestimmung von Metadatenqualität wie Wiederverwendung, Reproduzierbarkeit, Skalierbarkeit und Anpassungsfähigkeit.

Europeana.eu -- die europäische digitale Plattform für kulturelles Erbe -- sammelt Metadaten von 58 Millionen kulturellen Objekten, die aus mehr als 3200 Bibliotheken, Museen, Archiven und audiovisuellen Archiven in Europa stammen. Diese Sammlung ist heterogen und besteht aus Objekten in verschiedenen Formaten und Sprachen deren Beschreibungen durch unterschiedliche Indexierpraktiken entstanden sind. Oft wurden die Objekte aus ihrem ursprünglichen Kontext genommen. Um nun Dienstleistungen zu entwickeln, mit denen die Daten zugänglich gemacht und genutzt werden können, muss man die Stärken und Schwächen oder anders ausgedrückt die Qualität der Daten kennen. Der Bedarf an qualitativ hochwertigen Daten ist motiviert durch deren Einfluss auf die Nutzererfahrung, das Information Retrieval und die Wiederverwendung von Daten in anderen Zusammenhängen. Der Autor schlägt eine Methode und Open Source Lösung vor um strukturelle Eigenschaften wie Vollständigkeit, Multilingualität und Eindeutigkeit zu messen. Die Analyse und Verdeutlichung von Dokumentprofilen ist ein weitere Aspekt um Probleme in Daten aufzudecken.

Eines der Hauptziele von Europeana ist es NutzerInnen zu ermöglichen Kulturgüter unabhängig ihrer Herkunft und der Sprache in der sie beschrieben sind zu finden. Mehrsprachige Metadatenbeschreibungen sind daher unerlässlich um erfolgreiches sprachübergreifendes retrieval anzubieten. Die Kenntnis über die sprachliche Abdeckung Europeanas ist eine Vroraussetzung um überhaupt die Metadatenqualität in verschiedenen Sprachen zu verbessern. Um mehrsprachige Aspekte der Daten zu erfassen, müssen wir den kompletten Prozess der Datenaggregation abbilden können und Prozesse zur Datenverbesserung wie beispielsweise automatische Datenanreicherungen in Betracht ziehen. Zusammen mit Mitgliedern des Europeana Data Quality Committees, zeigt der Autor eine Methode um Mehrsprachigkeit als Aspekt verschiedener Dimensionen von Datenqualität, wie Vollständigkeit, Konsistenz, Konformität und Zugänglichkeit, messen zu können.

Das nächste Kapitel (Kapitel 4) geht darauf ein, wie der oben beschriebene Prozess umgesetzt werden kann und beschreibt die dahingehende Methode und die Ergebnisse ihrer Validierung mit 16 Bibliothekskatalogen. Die Katalogsdatei liegt im Machine Readable Cataloging (MARC21)-Format vor, dem am weitesten verbreiteten Metadatenstandard zur Beschreibung von Büchern. Die Forschung untersucht die strukturellen Merkmale der Daten auf deren Basis häufig auftretende Probleme gefunden und klassifiziert werden. Die häufigsten Probleme sind die Verwendung von undokumentierten Schema-Elementen, falsche Werte an Stellen, an denen ein Wert aus einem kontrollierten Vokabular? hätte übernommen werden sollen oder andere strenge Anforderungen erfüllt werden müssen.

Die nächsten Kapitel beschreiben die technischen Aspekte der Forschung. Zuerst wird ein kurzer Überblick über die Struktur ein erweiterbares Metadata Quality Assessment Framework gegeben, der verschiedene Metadatenschemata unterstützt und flexibel genug ist, um mit neuen Schemata umgehen zu können. Die Software muss skalierbar sein, um eine große Anzahl von Metadatensätzen innerhalb einer angemessenen Zeit verarbeiten zu können. Grundlegende Anforderungen, die bei der Entwicklung einer solchen Software berücksichtigt werden müssen, sind i) die Abstraktion des Metadatenschemas (im Rahmen des Messprozesses), ii) die Behandlung unterschiedlicher Teile innerhalb von Metadatensätzen, iii) der Messprozess, iv) ein gemeinsames und leistungsfähiges Interface für die einzelnen Metriken und v) die Interoperabilität mit Java- und REST-APIs. Zweitens wird aufgezeigt welche optimalen Parametereinstellungen für einen lang laufenden, zustandslosen Prozess basierend auf dem Standalone Modus von Apache Spark, nötig sind. Dieser misst die Auswirkungen von vier verschiedenen Parametern und vergleicht das Verhalten der Anwendung auf zwei verschiedenen Servern. Die wichtigsten Erkenntnisse aus diesem Experiment sind, dass die Zuweisung von mehr Ressourcen nicht unbedingt eine bessere Leistung bedeutet. Außerdem brauchen wir in einem Umfeld mit begrenzten und geteilten Ressourcen wirklich einen "gut genug" Zustand, der andere Prozessen den Vortritt lässt. Um die optimalen Einstellungen zu finden, sollte ein kleineres Sample herangezogen werden, das in wichtigen Merkmalen dem vollständigen Datensatz ähnelt, um die Leistung mit verschiedenen Einstellungen zu messen. Die Einstellungen, die es wert sind, überprüft zu werden, sind die Anzahl der Kerne, die Speicherzuweisung, die Kompression der Quelldateien und das Lesen aus verschiedenen Dateisystemen (falls vorhanden). Als Referenz können Spark Logdateien, sowie Event Logs oder Messpunkte innerhalb der Anwendung verwendet werden.

Das letzte Kapitel (Kapitel 7) erläutert Zukunftspläne, die Anwendbarkeit der Methode auf andere Subdomänen wie Wikicite (die offene Zitierdatenerfassung von Wikidata) und Forschungsdaten, sowie Forschungskooperationen mit verschiedenen Kulturerbeinstitutionen.
